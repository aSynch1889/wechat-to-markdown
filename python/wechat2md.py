#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« è½¬Markdownå·¥å…·
ä½¿ç”¨æ–¹æ³•ï¼špython wechat2md.py <æ–‡ç« URL>
æ‰¹é‡å¤„ç†ï¼špython wechat2md.py urls.txt
"""

import requests
from bs4 import BeautifulSoup
import html2text
import os
import re
import sys
from urllib.parse import urlparse
import time

class WechatToMarkdown:
    def __init__(self, output_dir='output'):
        self.output_dir = output_dir
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://mp.weixin.qq.com/'
        }
        self.session = requests.Session()
        
        # é…ç½®html2text
        self.h = html2text.HTML2Text()
        self.h.ignore_links = False
        self.h.ignore_images = False
        self.h.ignore_emphasis = False
        self.h.body_width = 0  # ä¸è‡ªåŠ¨æ¢è¡Œ
        self.h.unicode_snob = True
        self.h.skip_internal_links = False
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            print(f"âœ… åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")

    def get_article_content(self, url):
        """è·å–æ–‡ç« å†…å®¹"""
        try:
            print(f"ğŸ“¡ æ­£åœ¨è·å–æ–‡ç« : {url}")
            response = self.session.get(url, headers=self.headers, timeout=30)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                raise Exception(f"HTTP {response.status_code}")
            
            return response.text
        except Exception as e:
            raise Exception(f"è·å–æ–‡ç« å¤±è´¥: {str(e)}")

    def extract_article_info(self, html):
        """æå–æ–‡ç« ä¿¡æ¯"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # æå–æ ‡é¢˜
        title = None
        title_tag = soup.find('h1', class_='rich_media_title')
        if title_tag:
            title = title_tag.get_text().strip()
        
        if not title:
            title_tag = soup.find('title')
            if title_tag:
                title = title_tag.get_text().strip()
        
        if not title:
            title = "æœªå‘½åæ–‡ç« "
        
        # æå–ä½œè€…
        author = None
        author_tag = soup.find('a', class_='rich_media_meta_link')
        if author_tag:
            author = author_tag.get_text().strip()
        
        # æå–å‘å¸ƒæ—¶é—´
        publish_time = None
        time_tag = soup.find('em', id='publish_time')
        if time_tag:
            publish_time = time_tag.get_text().strip()
        
        # æå–æ­£æ–‡å†…å®¹
        content = None
        content_tag = soup.find('div', class_='rich_media_content')
        if content_tag:
            content = str(content_tag)
        
        if not content:
            raise Exception("æœªæ‰¾åˆ°æ–‡ç« æ­£æ–‡å†…å®¹")
        
        return {
            'title': title,
            'author': author,
            'publish_time': publish_time,
            'content': content
        }

    def clean_filename(self, filename):
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
        # ç§»é™¤æˆ–æ›¿æ¢éæ³•å­—ç¬¦
        filename = re.sub(r'[\\/*?:"<>|]', '', filename)
        # é™åˆ¶é•¿åº¦
        if len(filename) > 100:
            filename = filename[:100]
        return filename

    def download_images(self, content, article_dir):
        """ä¸‹è½½æ–‡ç« ä¸­çš„å›¾ç‰‡ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰"""
        soup = BeautifulSoup(content, 'html.parser')
        images = soup.find_all('img')
        
        img_dir = os.path.join(article_dir, 'images')
        if images and not os.path.exists(img_dir):
            os.makedirs(img_dir)
        
        for idx, img in enumerate(images):
            data_src = img.get('data-src')
            src = img.get('src')
            img_url = data_src or src
            
            if img_url:
                try:
                    print(f"  ğŸ“¥ ä¸‹è½½å›¾ç‰‡ {idx + 1}/{len(images)}")
                    img_response = self.session.get(img_url, headers=self.headers, timeout=15)
                    if img_response.status_code == 200:
                        # ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶å
                        ext = os.path.splitext(urlparse(img_url).path)[1] or '.jpg'
                        img_filename = f"image_{idx + 1}{ext}"
                        img_path = os.path.join(img_dir, img_filename)
                        
                        with open(img_path, 'wb') as f:
                            f.write(img_response.content)
                        
                        # æ›¿æ¢HTMLä¸­çš„å›¾ç‰‡é“¾æ¥ä¸ºæœ¬åœ°è·¯å¾„
                        img['src'] = f"images/{img_filename}"
                        if data_src:
                            del img['data-src']
                    time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
                except Exception as e:
                    print(f"  âš ï¸  å›¾ç‰‡ä¸‹è½½å¤±è´¥: {str(e)}")
        
        return str(soup)

    def convert_to_markdown(self, article_info, download_imgs=False):
        """è½¬æ¢ä¸ºMarkdownæ ¼å¼"""
        content = article_info['content']
        
        # åˆ›å»ºæ–‡ç« ä¸“å±ç›®å½•ï¼ˆå¦‚æœéœ€è¦ä¸‹è½½å›¾ç‰‡ï¼‰
        if download_imgs:
            article_dir = os.path.join(self.output_dir, self.clean_filename(article_info['title']))
            if not os.path.exists(article_dir):
                os.makedirs(article_dir)
            content = self.download_images(content, article_dir)
        else:
            article_dir = self.output_dir
        
        # è½¬æ¢ä¸ºMarkdown
        markdown_content = self.h.handle(content)
        
        # æ„å»ºå®Œæ•´çš„Markdownæ–‡æ¡£
        markdown = f"# {article_info['title']}\n\n"
        
        if article_info['author']:
            markdown += f"**ä½œè€…**: {article_info['author']}\n\n"
        
        if article_info['publish_time']:
            markdown += f"**å‘å¸ƒæ—¶é—´**: {article_info['publish_time']}\n\n"
        
        markdown += "---\n\n"
        markdown += markdown_content
        
        return markdown, article_dir

    def save_markdown(self, markdown, filename, article_dir):
        """ä¿å­˜Markdownæ–‡ä»¶"""
        filepath = os.path.join(article_dir, f"{filename}.md")
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(markdown)
        
        print(f"âœ… ä¿å­˜æˆåŠŸ: {filepath}")
        return filepath

    def process_url(self, url, download_imgs=False):
        """å¤„ç†å•ä¸ªURL"""
        try:
            # è·å–æ–‡ç« å†…å®¹
            html = self.get_article_content(url)
            
            # æå–æ–‡ç« ä¿¡æ¯
            article_info = self.extract_article_info(html)
            print(f"ğŸ“„ æ–‡ç« æ ‡é¢˜: {article_info['title']}")
            
            # è½¬æ¢ä¸ºMarkdown
            markdown, article_dir = self.convert_to_markdown(article_info, download_imgs)
            
            # ä¿å­˜æ–‡ä»¶
            filename = self.clean_filename(article_info['title'])
            self.save_markdown(markdown, filename, article_dir)
            
            return True, article_info['title']
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {str(e)}")
            return False, str(e)

    def process_file(self, filepath, download_imgs=False):
        """æ‰¹é‡å¤„ç†æ–‡ä»¶ä¸­çš„URLåˆ—è¡¨"""
        with open(filepath, 'r', encoding='utf-8') as f:
            urls = [line.strip() for line in f if line.strip()]
        
        print(f"ğŸ“‹ å…±æ‰¾åˆ° {len(urls)} ä¸ªé“¾æ¥\n")
        
        success_count = 0
        fail_count = 0
        
        for idx, url in enumerate(urls, 1):
            print(f"\n[{idx}/{len(urls)}] å¤„ç†ä¸­...")
            success, result = self.process_url(url, download_imgs)
            
            if success:
                success_count += 1
            else:
                fail_count += 1
            
            # å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            if idx < len(urls):
                time.sleep(2)
        
        print(f"\n{'='*50}")
        print(f"âœ… æˆåŠŸ: {success_count} ç¯‡")
        print(f"âŒ å¤±è´¥: {fail_count} ç¯‡")
        print(f"{'='*50}")


def main():
    print("="*50)
    print("å¾®ä¿¡å…¬ä¼—å·æ–‡ç« è½¬Markdownå·¥å…·")
    print("="*50 + "\n")
    
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  å•ä¸ªé“¾æ¥: python wechat2md.py <URL>")
        print("  æ‰¹é‡å¤„ç†: python wechat2md.py <æ–‡ä»¶è·¯å¾„>")
        print("\né€‰é¡¹:")
        print("  --download-images  ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°")
        print("\nç¤ºä¾‹:")
        print("  python wechat2md.py https://mp.weixin.qq.com/s/xxxxx")
        print("  python wechat2md.py urls.txt")
        print("  python wechat2md.py urls.txt --download-images")
        sys.exit(1)
    
    input_arg = sys.argv[1]
    download_imgs = '--download-images' in sys.argv
    
    converter = WechatToMarkdown()
    
    # åˆ¤æ–­æ˜¯URLè¿˜æ˜¯æ–‡ä»¶
    if input_arg.startswith('http'):
        # å•ä¸ªURL
        converter.process_url(input_arg, download_imgs)
    elif os.path.isfile(input_arg):
        # æ‰¹é‡å¤„ç†æ–‡ä»¶
        converter.process_file(input_arg, download_imgs)
    else:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶æˆ–æ— æ•ˆçš„URL: {input_arg}")
        sys.exit(1)


if __name__ == '__main__':
    main()